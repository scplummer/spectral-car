{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 8x8 grid graph (64 nodes)\n",
      "\n",
      "True parameters:\n",
      "  beta: [ 2.  -1.   0.5]\n",
      "  tau^2: 0.500\n",
      "  sigma^2: 0.250\n",
      "\n",
      "Fitting variational Bayes model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanp/spectral-car/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:1340: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
      "  current = float(metrics)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter    0 | ELBO:  -181.89 | E[log p(y|.)]:  -167.17 | KL:  14.72 | LL_std: 28.83 | MC: 20 | τ²: 0.490 | σ²: 0.245\n",
      "Iter  200 | ELBO:  -122.91 | E[log p(y|.)]:  -106.39 | KL:  16.51 | LL_std:  4.24 | MC: 20 | τ²: 0.500 | σ²: 0.250\n",
      "Iter  400 | ELBO:  -132.77 | E[log p(y|.)]:  -116.25 | KL:  16.51 | LL_std: 29.53 | MC: 20 | τ²: 0.500 | σ²: 0.250\n",
      "Iter  600 | ELBO:  -123.91 | E[log p(y|.)]:  -107.43 | KL:  16.48 | LL_std:  5.63 | MC: 23 | τ²: 0.500 | σ²: 0.250\n",
      "Iter  800 | ELBO:  -126.27 | E[log p(y|.)]:  -109.54 | KL:  16.73 | LL_std:  7.37 | MC: 29 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 1000 | ELBO:  -126.34 | E[log p(y|.)]:  -109.60 | KL:  16.73 | LL_std:  8.62 | MC: 36 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 1200 | ELBO:  -127.32 | E[log p(y|.)]:  -110.88 | KL:  16.44 | LL_std: 16.46 | MC: 42 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 1400 | ELBO:  -124.82 | E[log p(y|.)]:  -108.35 | KL:  16.48 | LL_std:  7.46 | MC: 48 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 1600 | ELBO:  -124.88 | E[log p(y|.)]:  -108.42 | KL:  16.46 | LL_std:  7.62 | MC: 55 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 1800 | ELBO:  -124.03 | E[log p(y|.)]:  -107.66 | KL:  16.38 | LL_std:  7.75 | MC: 61 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 2000 | ELBO:  -124.04 | E[log p(y|.)]:  -107.58 | KL:  16.46 | LL_std:  5.75 | MC: 68 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 2200 | ELBO:  -123.47 | E[log p(y|.)]:  -106.77 | KL:  16.71 | LL_std:  4.65 | MC: 74 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 2400 | ELBO:  -125.98 | E[log p(y|.)]:  -109.19 | KL:  16.80 | LL_std:  7.86 | MC: 80 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 2600 | ELBO:  -123.21 | E[log p(y|.)]:  -106.59 | KL:  16.62 | LL_std:  4.45 | MC: 87 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 2800 | ELBO:  -125.23 | E[log p(y|.)]:  -108.61 | KL:  16.61 | LL_std:  8.05 | MC: 93 | τ²: 0.500 | σ²: 0.250\n",
      "Iter 2999 | ELBO:  -124.39 | E[log p(y|.)]:  -107.68 | KL:  16.71 | LL_std:  7.16 | MC: 99 | τ²: 0.500 | σ²: 0.250\n",
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "\n",
      "Estimated parameters (mean ± std):\n",
      "  beta:\n",
      "    β_0:  2.034 ± 0.115  (true:  2.000)\n",
      "    β_1: -0.693 ± 0.124  (true: -1.000)\n",
      "    β_2:  0.438 ± 0.148  (true:  0.500)\n",
      "\n",
      "  tau^2:    0.500 ± 0.500  (true: 0.500)\n",
      "  sigma^2:  0.250 ± 0.144  (true: 0.250)\n",
      "\n",
      "Spatial effect prediction:\n",
      "  Correlation with truth: 0.890\n",
      "  Mean absolute error: 0.458\n",
      "  RMSE: 0.532\n",
      "\n",
      "Convergence:\n",
      "  Final ELBO: -124.39\n",
      "  Best ELBO: -121.07\n",
      "  Final log-likelihood std: 7.157\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import MultivariateNormal, InverseGamma, Normal\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SpectralCARVariationalBayes(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Bayes for spatial model with spectral polynomial CAR prior.\n",
    "    \n",
    "    Model:\n",
    "        y = X*beta + phi + eps, eps ~ N(0, sigma^2*I)\n",
    "        phi ~ N(0, (tau^2 * Q(theta))^-1)\n",
    "        Q(theta) = U * diag(p(lambda_j; theta)) * U^T\n",
    "        p(lambda; theta) = exp(sum_k theta_k * T_k(lambda))  [Chebyshev]\n",
    "    \n",
    "    Marginalizes out phi for computational efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_obs: int,\n",
    "        n_features: int,\n",
    "        eigenvalues: torch.Tensor,\n",
    "        eigenvectors: torch.Tensor,\n",
    "        poly_order: int = 5,\n",
    "        n_mc_samples: int = 20,\n",
    "        prior_beta_mean: Optional[torch.Tensor] = None,\n",
    "        prior_beta_std: float = 10.0,\n",
    "        prior_theta_mean: Optional[torch.Tensor] = None,\n",
    "        prior_theta_std: float = 1.0,\n",
    "        prior_tau_a: float = 3.0,\n",
    "        prior_tau_b: float = 1.0,\n",
    "        prior_sigma_a: float = 3.0,\n",
    "        prior_sigma_b: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_obs: Number of observations\n",
    "            n_features: Number of fixed effect features\n",
    "            eigenvalues: Eigenvalues of graph Laplacian (n_obs,)\n",
    "            eigenvectors: Eigenvectors of graph Laplacian (n_obs, n_obs)\n",
    "            poly_order: Order of Chebyshev polynomial\n",
    "            n_mc_samples: Number of MC samples for ELBO approximation\n",
    "            prior_*: Hyperparameters for priors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_obs = n_obs\n",
    "        self.n_features = n_features\n",
    "        self.poly_order = poly_order\n",
    "        self.n_mc_samples_initial = n_mc_samples\n",
    "        self.n_mc_samples = n_mc_samples\n",
    "        \n",
    "        # Store eigendecomposition\n",
    "        self.register_buffer('eigenvalues', eigenvalues)\n",
    "        self.register_buffer('eigenvectors', eigenvectors)\n",
    "        \n",
    "        # Normalize eigenvalues to [-1, 1] for Chebyshev stability\n",
    "        lambda_min = eigenvalues.min()\n",
    "        lambda_max = eigenvalues.max()\n",
    "        self.register_buffer('lambda_min', lambda_min)\n",
    "        self.register_buffer('lambda_max', lambda_max)\n",
    "        self.register_buffer(\n",
    "            'eigenvalues_normalized',\n",
    "            2 * (eigenvalues - lambda_min) / (lambda_max - lambda_min + 1e-8) - 1\n",
    "        )\n",
    "        \n",
    "        # Prior hyperparameters\n",
    "        self.register_buffer('prior_beta_mean', \n",
    "                           prior_beta_mean if prior_beta_mean is not None \n",
    "                           else torch.zeros(n_features))\n",
    "        self.register_buffer('prior_beta_cov', \n",
    "                           torch.eye(n_features) * prior_beta_std**2)\n",
    "        \n",
    "        self.register_buffer('prior_theta_mean',\n",
    "                           prior_theta_mean if prior_theta_mean is not None\n",
    "                           else torch.zeros(poly_order + 1))\n",
    "        self.register_buffer('prior_theta_cov',\n",
    "                           torch.eye(poly_order + 1) * prior_theta_std**2)\n",
    "        \n",
    "        self.prior_tau_a = prior_tau_a\n",
    "        self.prior_tau_b = prior_tau_b\n",
    "        self.prior_sigma_a = prior_sigma_a\n",
    "        self.prior_sigma_b = prior_sigma_b\n",
    "        \n",
    "        # Variational parameters (will be optimized)\n",
    "        self._init_variational_parameters()\n",
    "        \n",
    "    def _init_variational_parameters(self):\n",
    "        \"\"\"Initialize variational distribution parameters with better starting values.\"\"\"\n",
    "        # q(beta) = N(mu_beta, Sigma_beta)\n",
    "        self.mu_beta = nn.Parameter(torch.zeros(self.n_features))\n",
    "        self.log_diag_sigma_beta = nn.Parameter(torch.zeros(self.n_features))\n",
    "        \n",
    "        # q(theta) = N(mu_theta, Sigma_theta)\n",
    "        # Initialize with small positive first coefficient (smoother spatial field)\n",
    "        init_theta = torch.zeros(self.poly_order + 1)\n",
    "        init_theta[0] = 0.5  # Bias towards positive spectral density\n",
    "        self.mu_theta = nn.Parameter(init_theta)\n",
    "        self.log_diag_sigma_theta = nn.Parameter(torch.ones(self.poly_order + 1) * (-2))\n",
    "        \n",
    "        # q(tau^2) = InverseGamma(a_tau, b_tau)\n",
    "        # Initialize closer to expected true values\n",
    "        # For InvGamma(a,b): E[X] = b/(a-1), so b = E[X]*(a-1)\n",
    "        init_a_tau = 5.0\n",
    "        init_b_tau = (init_a_tau - 1) * 0.5  # Target E[tau²] ≈ 0.5\n",
    "        self.log_a_tau = nn.Parameter(torch.log(torch.tensor(init_a_tau)))\n",
    "        self.log_b_tau = nn.Parameter(torch.log(torch.tensor(init_b_tau)))\n",
    "        \n",
    "        # q(sigma^2) = InverseGamma(a_sigma, b_sigma)\n",
    "        init_a_sigma = 6.0  # Higher shape for more concentration\n",
    "        init_b_sigma = (init_a_sigma - 1) * 0.25  # Target E[sigma²] ≈ 0.25\n",
    "        self.log_a_sigma = nn.Parameter(torch.log(torch.tensor(init_a_sigma)))\n",
    "        self.log_b_sigma = nn.Parameter(torch.log(torch.tensor(init_b_sigma)))\n",
    "    \n",
    "    @property\n",
    "    def sigma_beta(self) -> torch.Tensor:\n",
    "        \"\"\"Standard deviations for beta (diagonal covariance).\"\"\"\n",
    "        return torch.exp(self.log_diag_sigma_beta)\n",
    "    \n",
    "    @property\n",
    "    def sigma_theta(self) -> torch.Tensor:\n",
    "        \"\"\"Standard deviations for theta (diagonal covariance).\"\"\"\n",
    "        return torch.exp(self.log_diag_sigma_theta)\n",
    "    \n",
    "    @property\n",
    "    def a_tau(self) -> torch.Tensor:\n",
    "        \"\"\"Shape parameter for tau^2.\"\"\"\n",
    "        return torch.exp(self.log_a_tau)\n",
    "    \n",
    "    @property\n",
    "    def b_tau(self) -> torch.Tensor:\n",
    "        \"\"\"Rate parameter for tau^2.\"\"\"\n",
    "        return torch.exp(self.log_b_tau)\n",
    "    \n",
    "    @property\n",
    "    def a_sigma(self) -> torch.Tensor:\n",
    "        \"\"\"Shape parameter for sigma^2.\"\"\"\n",
    "        return torch.exp(self.log_a_sigma)\n",
    "    \n",
    "    @property\n",
    "    def b_sigma(self) -> torch.Tensor:\n",
    "        \"\"\"Rate parameter for sigma^2.\"\"\"\n",
    "        return torch.exp(self.log_b_sigma)\n",
    "    \n",
    "    def chebyshev_polynomials(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Chebyshev polynomials T_0(x), ..., T_K(x).\n",
    "        \n",
    "        Args:\n",
    "            x: Input values, shape (n,)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (n, K+1) where [:, k] contains T_k(x)\n",
    "        \"\"\"\n",
    "        n = x.shape[0]\n",
    "        T = torch.zeros(n, self.poly_order + 1, device=x.device)\n",
    "        T[:, 0] = 1.0\n",
    "        if self.poly_order >= 1:\n",
    "            T[:, 1] = x\n",
    "        for k in range(2, self.poly_order + 1):\n",
    "            T[:, k] = 2 * x * T[:, k-1] - T[:, k-2]\n",
    "        return T\n",
    "    \n",
    "    def spectral_density(self, theta: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute p(lambda_j; theta) = exp(sum_k theta_k * T_k(lambda_j)).\n",
    "        \n",
    "        Args:\n",
    "            theta: Polynomial coefficients, shape (K+1,) or (batch, K+1)\n",
    "            \n",
    "        Returns:\n",
    "            Spectral density values, shape (n_obs,) or (batch, n_obs)\n",
    "        \"\"\"\n",
    "        T = self.chebyshev_polynomials(self.eigenvalues_normalized)  # (n_obs, K+1)\n",
    "        \n",
    "        if theta.dim() == 1:\n",
    "            # Single sample: T @ theta\n",
    "            log_p = torch.matmul(T, theta)  # (n_obs,)\n",
    "        else:\n",
    "            # Batch of samples: T @ theta.T -> (n_obs, batch) -> transpose to (batch, n_obs)\n",
    "            log_p = torch.matmul(T, theta.T).T  # (batch, n_obs)\n",
    "        \n",
    "        return torch.exp(log_p)\n",
    "    \n",
    "    def sample_variational_params(self, n_samples: int) -> dict:\n",
    "        \"\"\"Sample from variational distributions.\"\"\"\n",
    "        # Sample beta\n",
    "        beta_samples = self.mu_beta + self.sigma_beta * torch.randn(\n",
    "            n_samples, self.n_features, device=self.mu_beta.device\n",
    "        )\n",
    "        \n",
    "        # Sample theta\n",
    "        theta_samples = self.mu_theta + self.sigma_theta * torch.randn(\n",
    "            n_samples, self.poly_order + 1, device=self.mu_theta.device\n",
    "        )\n",
    "        \n",
    "        # Sample tau^2 (using Inverse Gamma)\n",
    "        # E[X] = b/(a-1) for InvGamma(a,b), sample via 1/Gamma(a, 1/b)\n",
    "        gamma_samples = torch.distributions.Gamma(\n",
    "            self.a_tau, 1.0 / self.b_tau\n",
    "        ).sample((n_samples,))\n",
    "        tau2_samples = 1.0 / gamma_samples\n",
    "        \n",
    "        # Sample sigma^2\n",
    "        gamma_samples = torch.distributions.Gamma(\n",
    "            self.a_sigma, 1.0 / self.b_sigma\n",
    "        ).sample((n_samples,))\n",
    "        sigma2_samples = 1.0 / gamma_samples\n",
    "        \n",
    "        return {\n",
    "            'beta': beta_samples,\n",
    "            'theta': theta_samples,\n",
    "            'tau2': tau2_samples,\n",
    "            'sigma2': sigma2_samples\n",
    "        }\n",
    "    \n",
    "    def marginal_log_likelihood(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        X: torch.Tensor,\n",
    "        beta: torch.Tensor,\n",
    "        theta: torch.Tensor,\n",
    "        tau2: torch.Tensor,\n",
    "        sigma2: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute log p(y | beta, theta, tau^2, sigma^2) with phi marginalized.\n",
    "        \n",
    "        Uses spectral decomposition for O(n) computation.\n",
    "        \n",
    "        Args:\n",
    "            y: Observations (n_obs,)\n",
    "            X: Design matrix (n_obs, n_features)\n",
    "            beta, theta, tau2, sigma2: Parameter samples\n",
    "            \n",
    "        Returns:\n",
    "            Log likelihood value (scalar or batch)\n",
    "        \"\"\"\n",
    "        # Compute spectral density p(lambda_j; theta)\n",
    "        p_lambda = self.spectral_density(theta)  # (n_obs,) or (batch, n_obs)\n",
    "        \n",
    "        # Add small epsilon for numerical stability\n",
    "        eps = 1e-6\n",
    "        p_lambda = torch.clamp(p_lambda, min=eps)\n",
    "        \n",
    "        # Compute diagonal of precision matrix in spectral domain\n",
    "        # D_jj = tau^2 * p(lambda_j) / (sigma^2 * tau^2 * p(lambda_j) + 1)\n",
    "        if theta.dim() == 1:\n",
    "            # Single sample\n",
    "            denom = sigma2 * tau2 * p_lambda + 1.0\n",
    "            D_diag = tau2 * p_lambda / denom\n",
    "            \n",
    "            # Compute residuals\n",
    "            residual = y - X @ beta  # (n_obs,)\n",
    "            \n",
    "            # Transform to spectral domain\n",
    "            y_tilde = self.eigenvectors.T @ residual  # (n_obs,)\n",
    "            \n",
    "            # Log determinant: sum_j log(sigma^2 * tau^2 * p_j + 1) - n*log(tau^2) - sum_j log(p_j)\n",
    "            log_det = torch.sum(torch.log(denom)) - self.n_obs * torch.log(tau2) - torch.sum(torch.log(p_lambda))\n",
    "            \n",
    "            # Quadratic form: sum_j D_jj * y_tilde_j^2\n",
    "            quad_form = torch.sum(D_diag * y_tilde**2)\n",
    "            \n",
    "        else:\n",
    "            # Batch of samples\n",
    "            batch_size = theta.shape[0]\n",
    "            p_lambda = p_lambda.view(batch_size, self.n_obs)\n",
    "            tau2 = tau2.view(batch_size, 1)\n",
    "            sigma2 = sigma2.view(batch_size, 1)\n",
    "            \n",
    "            denom = sigma2 * tau2 * p_lambda + 1.0\n",
    "            D_diag = tau2 * p_lambda / denom\n",
    "            \n",
    "            # Compute residuals for each sample\n",
    "            # beta: (batch, n_features), X: (n_obs, n_features)\n",
    "            # Need: y - X @ beta for each sample\n",
    "            Xbeta = torch.matmul(beta, X.T)  # (batch, n_obs)\n",
    "            residual = y.unsqueeze(0) - Xbeta  # (batch, n_obs)\n",
    "            \n",
    "            # Transform to spectral domain\n",
    "            y_tilde = torch.matmul(residual, self.eigenvectors)  # (batch, n_obs)\n",
    "            \n",
    "            log_det = (torch.sum(torch.log(denom), dim=1) \n",
    "                      - self.n_obs * torch.log(tau2.squeeze()) \n",
    "                      - torch.sum(torch.log(p_lambda), dim=1))\n",
    "            \n",
    "            quad_form = torch.sum(D_diag * y_tilde**2, dim=1)\n",
    "        \n",
    "        log_lik = -0.5 * self.n_obs * np.log(2 * np.pi) - 0.5 * log_det - 0.5 * quad_form\n",
    "        return log_lik\n",
    "    \n",
    "    def kl_divergence_terms(self) -> dict:\n",
    "        \"\"\"Compute KL divergences between variational and prior distributions.\"\"\"\n",
    "        # KL(q(beta) || p(beta))\n",
    "        kl_beta = 0.5 * (\n",
    "            torch.sum(self.sigma_beta**2 / torch.diag(self.prior_beta_cov))\n",
    "            + torch.sum(((self.mu_beta - self.prior_beta_mean)**2) / torch.diag(self.prior_beta_cov))\n",
    "            - self.n_features\n",
    "            + torch.sum(torch.log(torch.diag(self.prior_beta_cov)))\n",
    "            - 2 * torch.sum(self.log_diag_sigma_beta)\n",
    "        )\n",
    "        \n",
    "        # KL(q(theta) || p(theta))\n",
    "        kl_theta = 0.5 * (\n",
    "            torch.sum(self.sigma_theta**2 / torch.diag(self.prior_theta_cov))\n",
    "            + torch.sum(((self.mu_theta - self.prior_theta_mean)**2) / torch.diag(self.prior_theta_cov))\n",
    "            - (self.poly_order + 1)\n",
    "            + torch.sum(torch.log(torch.diag(self.prior_theta_cov)))\n",
    "            - 2 * torch.sum(self.log_diag_sigma_theta)\n",
    "        )\n",
    "        \n",
    "        # KL(q(tau^2) || p(tau^2)) for InverseGamma\n",
    "        kl_tau = (\n",
    "            self.a_tau * torch.log(self.b_tau / self.prior_tau_b)\n",
    "            - torch.lgamma(self.a_tau) + torch.lgamma(torch.tensor(self.prior_tau_a))\n",
    "            + (self.prior_tau_a - self.a_tau) * (torch.log(self.b_tau) - torch.digamma(self.a_tau))\n",
    "            + self.a_tau * (self.prior_tau_b / self.b_tau - 1.0)\n",
    "        )\n",
    "        \n",
    "        # KL(q(sigma^2) || p(sigma^2))\n",
    "        kl_sigma = (\n",
    "            self.a_sigma * torch.log(self.b_sigma / self.prior_sigma_b)\n",
    "            - torch.lgamma(self.a_sigma) + torch.lgamma(torch.tensor(self.prior_sigma_a))\n",
    "            + (self.prior_sigma_a - self.a_sigma) * (torch.log(self.b_sigma) - torch.digamma(self.a_sigma))\n",
    "            + self.a_sigma * (self.prior_sigma_b / self.b_sigma - 1.0)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'kl_beta': kl_beta,\n",
    "            'kl_theta': kl_theta,\n",
    "            'kl_tau': kl_tau,\n",
    "            'kl_sigma': kl_sigma\n",
    "        }\n",
    "    \n",
    "    def elbo(self, y: torch.Tensor, X: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Compute the Evidence Lower Bound (ELBO) using MC approximation.\n",
    "        \n",
    "        Args:\n",
    "            y: Observations (n_obs,)\n",
    "            X: Design matrix (n_obs, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            ELBO value (scalar, to be maximized)\n",
    "            Dictionary of diagnostics\n",
    "        \"\"\"\n",
    "        # Sample from variational distributions\n",
    "        samples = self.sample_variational_params(self.n_mc_samples)\n",
    "        \n",
    "        # Compute expected log likelihood via MC\n",
    "        log_liks = self.marginal_log_likelihood(\n",
    "            y, X, \n",
    "            samples['beta'], \n",
    "            samples['theta'],\n",
    "            samples['tau2'],\n",
    "            samples['sigma2']\n",
    "        )\n",
    "        expected_log_lik = torch.mean(log_liks)\n",
    "        \n",
    "        # Compute KL divergences (analytical)\n",
    "        kl_terms = self.kl_divergence_terms()\n",
    "        total_kl = sum(kl_terms.values())\n",
    "        \n",
    "        # ELBO = E[log p(y|params)] - KL\n",
    "        elbo_value = expected_log_lik - total_kl\n",
    "        \n",
    "        diagnostics = {\n",
    "            'expected_log_lik': expected_log_lik.item(),\n",
    "            'total_kl': total_kl.item(),\n",
    "            'log_lik_std': torch.std(log_liks).item(),\n",
    "            **{k: v.item() for k, v in kl_terms.items()}\n",
    "        }\n",
    "        \n",
    "        return elbo_value, diagnostics\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        X: torch.Tensor,\n",
    "        n_iterations: int = 3000,\n",
    "        learning_rate: float = 0.03,\n",
    "        n_mc_samples_final: int = 100,\n",
    "        warmup_iterations: int = 500,\n",
    "        use_scheduler: bool = True,\n",
    "        verbose: bool = True,\n",
    "        print_every: int = 100\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fit the model using natural gradient variational inference.\n",
    "        \n",
    "        Args:\n",
    "            y: Observations (n_obs,)\n",
    "            X: Design matrix (n_obs, n_features)\n",
    "            n_iterations: Number of optimization iterations\n",
    "            learning_rate: Initial learning rate\n",
    "            n_mc_samples_final: Final number of MC samples (ramped up during training)\n",
    "            warmup_iterations: Number of iterations before ramping up MC samples\n",
    "            use_scheduler: Whether to use learning rate scheduler\n",
    "            verbose: Whether to print progress\n",
    "            print_every: Print frequency\n",
    "        \"\"\"\n",
    "        # Different learning rates for different parameter groups\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': [self.mu_beta, self.log_diag_sigma_beta], 'lr': learning_rate},\n",
    "            {'params': [self.mu_theta, self.log_diag_sigma_theta], 'lr': learning_rate * 0.8},\n",
    "            {'params': [self.log_a_tau, self.log_b_tau], 'lr': learning_rate * 0.3},\n",
    "            {'params': [self.log_a_sigma, self.log_b_sigma], 'lr': learning_rate * 0.3}\n",
    "        ])\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        if use_scheduler:\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='max', factor=0.7, patience=150, \n",
    "                threshold=0.01\n",
    "            )\n",
    "        \n",
    "        history = []\n",
    "        best_elbo = -float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            # Gradually increase MC samples after warmup\n",
    "            if iteration < warmup_iterations:\n",
    "                self.n_mc_samples = self.n_mc_samples_initial\n",
    "            else:\n",
    "                progress = (iteration - warmup_iterations) / (n_iterations - warmup_iterations)\n",
    "                self.n_mc_samples = int(\n",
    "                    self.n_mc_samples_initial + \n",
    "                    progress * (n_mc_samples_final - self.n_mc_samples_initial)\n",
    "                )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute negative ELBO (loss to minimize)\n",
    "            elbo_value, diagnostics = self.elbo(y, X)\n",
    "            loss = -elbo_value\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            if use_scheduler:\n",
    "                scheduler.step(elbo_value.detach())\n",
    "            \n",
    "            # Track best model (could implement early stopping here)\n",
    "            if elbo_value.item() > best_elbo:\n",
    "                best_elbo = elbo_value.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Logging\n",
    "            diagnostics['elbo'] = elbo_value.item()\n",
    "            diagnostics['iteration'] = iteration\n",
    "            diagnostics['n_mc_samples'] = self.n_mc_samples\n",
    "            diagnostics['learning_rate'] = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Add current parameter values to diagnostics\n",
    "            with torch.no_grad():\n",
    "                diagnostics['tau2_current'] = (self.b_tau / (self.a_tau - 1)).item()\n",
    "                diagnostics['sigma2_current'] = (self.b_sigma / (self.a_sigma - 1)).item()\n",
    "            \n",
    "            history.append(diagnostics)\n",
    "            \n",
    "            if verbose and (iteration % print_every == 0 or iteration == n_iterations - 1):\n",
    "                print(f\"Iter {iteration:4d} | ELBO: {elbo_value.item():8.2f} | \"\n",
    "                      f\"E[log p(y|.)]: {diagnostics['expected_log_lik']:8.2f} | \"\n",
    "                      f\"KL: {diagnostics['total_kl']:6.2f} | \"\n",
    "                      f\"LL_std: {diagnostics['log_lik_std']:5.2f} | \"\n",
    "                      f\"MC: {self.n_mc_samples:2d} | \"\n",
    "                      f\"τ²: {diagnostics['tau2_current']:.3f} | \"\n",
    "                      f\"σ²: {diagnostics['sigma2_current']:.3f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict_spatial_effect(self, y: torch.Tensor, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predict the spatial random effect phi given data.\n",
    "        \n",
    "        Returns mean and standard deviation.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Use posterior mean of hyperparameters\n",
    "            beta = self.mu_beta\n",
    "            theta = self.mu_theta\n",
    "            tau2 = self.b_tau / (self.a_tau - 1)  # E[tau^2]\n",
    "            sigma2 = self.b_sigma / (self.a_sigma - 1)  # E[sigma^2]\n",
    "            \n",
    "            # Compute residuals\n",
    "            residual = y - X @ beta\n",
    "            \n",
    "            # Transform to spectral domain\n",
    "            y_tilde = self.eigenvectors.T @ residual\n",
    "            \n",
    "            # Compute spectral density\n",
    "            p_lambda = self.spectral_density(theta)\n",
    "            p_lambda = torch.clamp(p_lambda, min=1e-6)\n",
    "            \n",
    "            # Posterior mean in spectral domain: M_jj * y_tilde_j\n",
    "            # M_jj = 1 / (sigma^2 * tau^2 * p_j + 1)\n",
    "            M_diag = 1.0 / (sigma2 * tau2 * p_lambda + 1.0)\n",
    "            alpha_mean = M_diag * y_tilde\n",
    "            \n",
    "            # Transform back to spatial domain\n",
    "            phi_mean = self.eigenvectors @ alpha_mean\n",
    "            \n",
    "            # Posterior variance (diagonal approximation)\n",
    "            phi_var = self.eigenvectors @ (M_diag.unsqueeze(-1) * self.eigenvectors.T)\n",
    "            phi_std = torch.sqrt(torch.diag(phi_var))\n",
    "            \n",
    "            return phi_mean, phi_std\n",
    "    \n",
    "    def get_parameter_summary(self) -> dict:\n",
    "        \"\"\"Get summary of estimated parameters.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Check if a > 2 for valid variance computation\n",
    "            tau2_mean = (self.b_tau / (self.a_tau - 1)).item()\n",
    "            tau2_var = self.b_tau**2 / ((self.a_tau - 1)**2 * torch.clamp(self.a_tau - 2, min=1e-6))\n",
    "            tau2_std = torch.sqrt(tau2_var).item()\n",
    "            \n",
    "            sigma2_mean = (self.b_sigma / (self.a_sigma - 1)).item()\n",
    "            sigma2_var = self.b_sigma**2 / ((self.a_sigma - 1)**2 * torch.clamp(self.a_sigma - 2, min=1e-6))\n",
    "            sigma2_std = torch.sqrt(sigma2_var).item()\n",
    "            \n",
    "            return {\n",
    "                'beta_mean': self.mu_beta.cpu().numpy(),\n",
    "                'beta_std': self.sigma_beta.cpu().numpy(),\n",
    "                'theta_mean': self.mu_theta.cpu().numpy(),\n",
    "                'theta_std': self.sigma_theta.cpu().numpy(),\n",
    "                'tau2_mean': tau2_mean,\n",
    "                'tau2_std': tau2_std,\n",
    "                'sigma2_mean': sigma2_mean,\n",
    "                'sigma2_std': sigma2_std,\n",
    "            }\n",
    "\n",
    "\n",
    "def create_example_graph_laplacian(n_nodes: int, grid_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create a simple grid graph Laplacian for demonstration.\n",
    "    \n",
    "    Args:\n",
    "        n_nodes: Number of nodes (should be grid_size^2)\n",
    "        grid_size: Size of square grid\n",
    "        \n",
    "    Returns:\n",
    "        eigenvalues, eigenvectors of the graph Laplacian\n",
    "    \"\"\"\n",
    "    # Create adjacency matrix for grid graph\n",
    "    W = torch.zeros(n_nodes, n_nodes)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            idx = i * grid_size + j\n",
    "            # Right neighbor\n",
    "            if j < grid_size - 1:\n",
    "                W[idx, idx + 1] = 1\n",
    "                W[idx + 1, idx] = 1\n",
    "            # Bottom neighbor\n",
    "            if i < grid_size - 1:\n",
    "                W[idx, idx + grid_size] = 1\n",
    "                W[idx + grid_size, idx] = 1\n",
    "    \n",
    "    # Degree matrix\n",
    "    D = torch.diag(W.sum(dim=1))\n",
    "    \n",
    "    # Graph Laplacian\n",
    "    L = D - W\n",
    "    \n",
    "    # Eigendecomposition\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(L)\n",
    "    \n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Setup\n",
    "    grid_size = 8\n",
    "    n_obs = grid_size ** 2\n",
    "    n_features = 3\n",
    "    \n",
    "    print(f\"Creating {grid_size}x{grid_size} grid graph ({n_obs} nodes)\")\n",
    "    \n",
    "    # Create graph structure\n",
    "    eigenvalues, eigenvectors = create_example_graph_laplacian(n_obs, grid_size)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    X = torch.randn(n_obs, n_features)\n",
    "    X[:, 0] = 1.0  # Intercept\n",
    "    \n",
    "    true_beta = torch.tensor([2.0, -1.0, 0.5])\n",
    "    \n",
    "    # True spatial effect (smooth pattern)\n",
    "    true_theta = torch.tensor([0.5, -1.0, 0.3, 0.0, 0.0, 0.0])\n",
    "    true_tau2 = 0.5\n",
    "    true_sigma2 = 0.25\n",
    "    \n",
    "    model_true = SpectralCARVariationalBayes(\n",
    "        n_obs, n_features, eigenvalues, eigenvectors, poly_order=5\n",
    "    )\n",
    "    true_p_lambda = model_true.spectral_density(true_theta)\n",
    "    \n",
    "    # Sample spatial effect\n",
    "    Q_inv = eigenvectors @ torch.diag(1.0 / (true_tau2 * true_p_lambda)) @ eigenvectors.T\n",
    "    phi_true = torch.distributions.MultivariateNormal(\n",
    "        torch.zeros(n_obs), Q_inv\n",
    "    ).sample()\n",
    "    \n",
    "    # Generate observations\n",
    "    y = X @ true_beta + phi_true + torch.randn(n_obs) * torch.sqrt(torch.tensor(true_sigma2))\n",
    "    \n",
    "    print(f\"\\nTrue parameters:\")\n",
    "    print(f\"  beta: {true_beta.numpy()}\")\n",
    "    print(f\"  tau^2: {true_tau2:.3f}\")\n",
    "    print(f\"  sigma^2: {true_sigma2:.3f}\")\n",
    "    \n",
    "    # Fit model with improved settings\n",
    "    print(f\"\\nFitting variational Bayes model...\")\n",
    "    model = SpectralCARVariationalBayes(\n",
    "        n_obs=n_obs,\n",
    "        n_features=n_features,\n",
    "        eigenvalues=eigenvalues,\n",
    "        eigenvectors=eigenvectors,\n",
    "        poly_order=5,\n",
    "        n_mc_samples=20,\n",
    "        prior_tau_a=3.0,\n",
    "        prior_tau_b=1.0,\n",
    "        prior_sigma_a=5.0,  # Stronger prior\n",
    "        prior_sigma_b=1.0   # Tighter around E[sigma²] = 0.25\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        y, X, \n",
    "        n_iterations=3000,\n",
    "        learning_rate=0.03,\n",
    "        n_mc_samples_final=100,\n",
    "        warmup_iterations=500,\n",
    "        use_scheduler=True,\n",
    "        verbose=True, \n",
    "        print_every=200\n",
    "    )\n",
    "    \n",
    "    # Results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    param_summary = model.get_parameter_summary()\n",
    "    \n",
    "    print(f\"\\nEstimated parameters (mean ± std):\")\n",
    "    print(f\"  beta:\")\n",
    "    for i, (m, s, t) in enumerate(zip(param_summary['beta_mean'], \n",
    "                                       param_summary['beta_std'],\n",
    "                                       true_beta.numpy())):\n",
    "        print(f\"    β_{i}: {m:6.3f} ± {s:5.3f}  (true: {t:6.3f})\")\n",
    "    \n",
    "    print(f\"\\n  tau^2:   {param_summary['tau2_mean']:6.3f} ± {param_summary['tau2_std']:5.3f}  (true: {true_tau2:.3f})\")\n",
    "    print(f\"  sigma^2: {param_summary['sigma2_mean']:6.3f} ± {param_summary['sigma2_std']:5.3f}  (true: {true_sigma2:.3f})\")\n",
    "    \n",
    "    # Predict spatial effects\n",
    "    phi_mean, phi_std = model.predict_spatial_effect(y, X)\n",
    "    \n",
    "    print(f\"\\nSpatial effect prediction:\")\n",
    "    print(f\"  Correlation with truth: {torch.corrcoef(torch.stack([phi_true, phi_mean]))[0,1].item():.3f}\")\n",
    "    print(f\"  Mean absolute error: {torch.mean(torch.abs(phi_true - phi_mean)).item():.3f}\")\n",
    "    print(f\"  RMSE: {torch.sqrt(torch.mean((phi_true - phi_mean)**2)).item():.3f}\")\n",
    "    \n",
    "    # Print final ELBO convergence\n",
    "    print(f\"\\nConvergence:\")\n",
    "    print(f\"  Final ELBO: {history[-1]['elbo']:.2f}\")\n",
    "    print(f\"  Best ELBO: {max([h['elbo'] for h in history]):.2f}\")\n",
    "    print(f\"  Final log-likelihood std: {history[-1]['log_lik_std']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
